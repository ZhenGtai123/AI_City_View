"""
é˜¶æ®µ2: AIæ¨¡å‹æ¨ç†
åŠŸèƒ½: è¯­ä¹‰åˆ†å‰² (SAM 2.1 + LangSAM) + æ·±åº¦ä¼°è®¡ (Depth Anything V2)
ç²¾åº¦ä¼˜å…ˆæ–¹æ¡ˆ
"""

from __future__ import annotations

import time
import numpy as np
import cv2
from typing import Dict, Any, List, Optional, Tuple
from PIL import Image
import threading

# å»¶è¿Ÿå¯¼å…¥ï¼Œé¿å…æœªå®‰è£…æ—¶ç›´æ¥æŠ¥é”™
try:
    import torch
    TORCH_AVAILABLE = True
except ImportError:
    TORCH_AVAILABLE = False
    print("è­¦å‘Š: PyTorchæœªå®‰è£…ï¼ŒAIæ¨ç†åŠŸèƒ½å°†ä¸å¯ç”¨")


# å…¨å±€GPUæ¨ç†é”: åªåŒ…è£¹torchæ¨ç†ä»£ç å—ï¼ŒCPUé¢„å¤„ç†/åå¤„ç†ä¸æŒé”
_GPU_INFERENCE_LOCK = threading.Lock()

# æ¨¡å‹åŠ è½½é”: é˜²æ­¢å¤šçº¿ç¨‹åŒæ—¶åŠ è½½æ¨¡å‹ï¼ˆç‰¹åˆ«æ˜¯å¤§æ¨¡å‹ä¼šå¯¼è‡´OOMï¼‰
_MODEL_LOAD_LOCK = threading.Lock()


def stage2_ai_inference(image: np.ndarray, config: Dict[str, Any]) -> Dict[str, Any]:
    """
    é˜¶æ®µ2: AIæ¨¡å‹æ¨ç†

    å‚æ•°:
        image: np.ndarray - æ¥è‡ªé˜¶æ®µ1çš„ original (H, W, 3) BGRæ ¼å¼
        config: dict - é…ç½®å‚æ•°

    è¿”å›:
        dict - åŒ…å«ä»¥ä¸‹é”®:
            - semantic_map: np.ndarray - è¯­ä¹‰åˆ†å‰²å›¾ (H, W) uint8 (è‹¥ç¦ç”¨åˆ™å…¨0)
            - depth_map: np.ndarray - æ·±åº¦å›¾ (H, W) uint8
    """
    H, W = image.shape[:2]

    # è¯­ä¹‰åˆ†å‰²ï¼ˆå¯ç¦ç”¨ï¼ŒDA3 NESTED å·²å†…ç½®å¤©ç©ºåˆ†å‰²ï¼‰
    if config.get('enable_semantic', True):
        semantic_map = _semantic_segmentation(image, config)
    else:
        semantic_map = np.zeros((H, W), dtype=np.uint8)

    # æ·±åº¦ä¼°è®¡ (è¿”å› uint8 å¯è§†åŒ– + float32 åŸå§‹ç±³æ•° + sky_mask)
    depth_result = _depth_estimation_with_metric(image, config)
    depth_map = depth_result['depth_map']
    depth_metric = depth_result['depth_metric']
    sky_mask = depth_result['sky_mask']

    # å¤©ç©ºå¤„ç†:
    # - DA3NESTED: depthæä¾›sky_mask â†’ ä¿®æ­£semantic_map
    # - DA3METRIC/MONO: æ— depth sky_mask â†’ ä»OneFormer semantic_map==2æ¨å¯¼
    if sky_mask is not None and sky_mask.any():
        # DA3NESTEDçš„å¤©ç©ºæ©ç ä¿®æ­£OneFormerçš„è¯­ä¹‰åˆ†å‰² (ADE20K sky=2)
        semantic_map[sky_mask] = 2
    elif depth_metric is not None:
        # DA3METRIC: ä»OneFormeræ¨å¯¼å¤©ç©ºæ©ç , è®¾ç½®depth_metricä¸ºinf
        semantic_sky = (semantic_map == 2)
        if semantic_sky.any():
            sky_mask = semantic_sky
            depth_metric[sky_mask] = np.inf
            sky_pct = sky_mask.sum() / sky_mask.size * 100
            print(f"  ğŸŒ¤ï¸  Sky mask (from OneFormer): {sky_pct:.1f}% pixels")

    # å¤©ç©ºç¼éš™ä¿®è¡¥: æ ‘ç¼/å»ºç­‘ç¼éš™é—´çš„å¤©ç©º OneFormer å®¹æ˜“æ¼æ‰
    # ç­–ç•¥: æ·±åº¦ > p95 (éå¤©ç©º) + é è¿‘å·²çŸ¥å¤©ç©ºåŒºåŸŸ (è†¨èƒ€æ©ç ) â†’ è¡¥å……ä¸ºå¤©ç©º
    if (config.get('sky_refine_gaps', True)
            and sky_mask is not None and sky_mask.any()
            and depth_metric is not None):
        sky_mask, gap_count = _refine_sky_gaps(
            depth_metric, sky_mask,
            image_bgr=image,
            semantic_map=semantic_map,
        )
        if gap_count > 0:
            semantic_map[sky_mask] = 2
            depth_metric[sky_mask] = np.inf
            print(f"  ğŸŒ¤ï¸  Sky gap refinement: +{gap_count} pixels "
                  f"(total {sky_mask.sum()/sky_mask.size*100:.1f}%)")

    return {
        'semantic_map': semantic_map,
        'depth_map': depth_map,
        'depth_metric': depth_metric,   # float32, å•ä½: ç±³
        'sky_mask': sky_mask,           # bool (H,W) or None
    }


def _semantic_segmentation(image: np.ndarray, config: Dict[str, Any]) -> np.ndarray:
    """
    è¯­ä¹‰åˆ†å‰² - ä½¿ç”¨ SAM 2.1 + LangSAM
    
    å‚æ•°:
        image: (H, W, 3) BGR uint8
        config: dict - é…ç½®å‚æ•°
    
    è¿”å›:
        semantic_map: (H, W) uint8, å€¼èŒƒå›´ [0, N]
            - 0: èƒŒæ™¯/æœªåˆ†ç±»
            - 1-N: è¯­ä¹‰ç±»åˆ«ID (N = len(classes))
    """
    H, W = image.shape[:2]
    backend = str(config.get('semantic_backend', 'oneformer_ade20k')).strip().lower()

    # åˆå§‹åŒ–è¯­ä¹‰åˆ†å‰²å›¾
    semantic_map = np.zeros((H, W), dtype=np.uint8)

    if backend.startswith('oneformer'):
        try:
            profile = bool(config.get('profile', False))
            t0 = time.perf_counter()

            model, processor = get_semantic_model(config)
            if model is None or processor is None:
                print("  âš ï¸  è­¦å‘Š: OneFormer æœªå°±ç»ªï¼Œè¯­ä¹‰åˆ†å‰²ä½¿ç”¨å ä½å®ç°")
                return semantic_map

            _maybe_apply_semantic_items_mapping_for_ade20k(model, config)

            if profile:
                print(f"  â±ï¸  OneFormer ready: {time.perf_counter() - t0:.3f}s")

            device = _get_torch_device(config)
            use_fp16 = bool(config.get('semantic_use_fp16', True))

            # OpenCV(BGR)->RGB
            rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)

            # OneFormer semantic segmentation
            t1 = time.perf_counter()
            with _GPU_INFERENCE_LOCK:
                with torch.inference_mode():
                    inputs = processor(images=rgb, task_inputs=["semantic"], return_tensors="pt")
                    inputs = {k: v.to(device) for k, v in inputs.items()}
                    if use_fp16 and device.type == 'cuda':
                        inputs = {k: v.half() if torch.is_floating_point(v) else v for k, v in inputs.items()}
                    outputs = model(**inputs)
                    pred = processor.post_process_semantic_segmentation(outputs, target_sizes=[(H, W)])[0]
                semantic_map = pred.detach().to('cpu').numpy().astype(np.uint8)
            if profile:
                print(f"  â±ï¸  OneFormer inference+post: {time.perf_counter() - t1:.3f}s")
            return semantic_map

        except Exception as e:
            print(f"  âŒ OneFormer è¯­ä¹‰åˆ†å‰²å‡ºé”™: {e}")
            import traceback
            traceback.print_exc()
            return semantic_map

    # å…¼å®¹æ—§å®ç°ï¼šLangSAM æ–‡æœ¬æç¤ºåˆ†å‰²
    classes = list(config.get('classes', []) or [])
    semantic_cfg = config.get('semantic', {}) or {}

    if len(classes) == 0:
        print("  è­¦å‘Š: æœªæŒ‡å®šç±»åˆ«ï¼Œè¿”å›å…¨0è¯­ä¹‰å›¾")
        return semantic_map

    try:
        profile = bool(config.get('profile', False))
        t0 = time.perf_counter()

        model, _processor = get_semantic_model({**config, 'semantic_backend': 'langsam'})
        if model is None:
            print("  âš ï¸  è­¦å‘Š: LangSAM æœªå°±ç»ªï¼Œè¯­ä¹‰åˆ†å‰²ä½¿ç”¨å ä½å®ç°")
            _generate_placeholder_semantic_map(semantic_map, classes, H, W)
            return semantic_map

        if profile:
            print(f"  â±ï¸  LangSAM ready: {time.perf_counter() - t0:.3f}s")

        # OpenCV(BGR)->RGB PIL
        rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)
        pil_img = Image.fromarray(rgb)

        for class_id, class_name in enumerate(classes, start=1):
            if profile:
                t_cls = time.perf_counter()
            masks = _langsam_predict_masks(model, pil_img, class_name, semantic_cfg)

            if masks is None:
                continue
            if masks.ndim == 2:
                combined = masks.astype(bool)
            else:
                combined = np.any(masks.astype(bool), axis=0)

            if combined.any():
                semantic_map[combined] = class_id

            if profile:
                print(f"  â±ï¸  semantic '{class_name}': {time.perf_counter() - t_cls:.3f}s")

    except Exception as e:
        print(f"  âŒ LangSAM è¯­ä¹‰åˆ†å‰²å‡ºé”™: {e}")
        import traceback
        traceback.print_exc()
        return semantic_map

    return semantic_map


def _normalize_label(s: str) -> str:
    import re

    s = s.lower().strip()
    # Normalize separators and punctuation.
    s = s.replace('&', ' and ')
    s = re.sub(r"[\[\]\(\)\{\}]+", " ", s)
    s = re.sub(r"[;,:/]+", " ", s)
    s = re.sub(r"\s+", " ", s).strip()
    return s


def _maybe_apply_semantic_items_mapping_for_ade20k(model, config: Dict[str, Any]) -> None:
    """Build colors/openness for OneFormer ADE20K ids from user-provided semantic_items.

    - OneFormer outputs class ids 0..149 (ADE20K-150)
    - User semantic config contains a small set of human labels with colors/openness.
    This function tries to match those labels to ADE20K id2label and fills:
      - config['classes'] as id-aligned label list (for stats)
      - config['colors'] mapping for matched ids (others handled by stage6 fallback)
      - config['openness_config'] list length max_id+1 with matched ids set
    """
    if config.get('_ade20k_mapped_from_semantic_items'):
        return
    backend = str(config.get('semantic_backend', 'oneformer_ade20k')).strip().lower()
    if not backend.startswith('oneformer'):
        return

    items = config.get('semantic_items', None)
    if not items:
        return

    id2label = getattr(getattr(model, 'config', None), 'id2label', None)
    if not isinstance(id2label, dict) or not id2label:
        return

    # Build id-aligned label list
    max_id = max(int(k) for k in id2label.keys() if str(k).isdigit()) if any(str(k).isdigit() for k in id2label.keys()) else 149
    labels_by_id = []
    for i in range(0, max_id + 1):
        labels_by_id.append(str(id2label.get(i, f"class_{i}")))

    # Build normalized label->id index
    norm_to_id = {}
    for i, lab in enumerate(labels_by_id):
        norm_to_id[_normalize_label(lab)] = i

    # æ³¨æ„: ä¸å†é»˜è®¤æŠŠid=0è®¾ä¸ºé»‘è‰²ï¼Œå› ä¸ºADE20Kä¸­0æ˜¯wall
    colors = {}
    openness_config = [0] * (max_id + 1)

    for item in items:
        if not isinstance(item, dict):
            continue
        name = str(item.get('name', '')).strip()
        if not name:
            continue
        openness = int(item.get('openness', 0) or 0)
        bgr = item.get('bgr', None)
        if bgr is None:
            continue

        # Try exact normalized match; then try synonyms split by ';'
        candidates = [name] + [p.strip() for p in name.split(';') if p.strip()]
        matched_id = None
        for cand in candidates:
            nid = norm_to_id.get(_normalize_label(cand), None)
            if nid is not None:
                matched_id = int(nid)
                break

        # Fallback: substring match (avoid ambiguous matches)
        if matched_id is None:
            n = _normalize_label(name)
            hits = [i for k, i in norm_to_id.items() if (n == k or n in k or k in n)]
            if len(hits) == 1:
                matched_id = int(hits[0])

        # æ³¨æ„: ADE20Kä¸­class 0æ˜¯wallï¼Œæ‰€ä»¥ä¸èƒ½æ’é™¤matched_id == 0
        if matched_id is None or matched_id < 0 or matched_id > max_id:
            continue

        colors[matched_id] = tuple(int(x) for x in bgr)
        openness_config[matched_id] = 1 if openness == 1 else 0

    config['classes'] = labels_by_id
    config['colors'] = colors
    config['openness_config'] = openness_config
    config['_ade20k_mapped_from_semantic_items'] = True


def _depth_estimation(image: np.ndarray, config: Dict[str, Any]) -> np.ndarray:
    """
    æ·±åº¦ä¼°è®¡ - æ”¯æŒ Depth Pro, Depth Anything V2/V3 (ä»…è¿”å› uint8)
    ä¿ç•™å‘åå…¼å®¹ï¼Œå†…éƒ¨è°ƒç”¨ _depth_estimation_with_metric
    """
    result = _depth_estimation_with_metric(image, config)
    return result['depth_map']


def _depth_estimation_with_metric(image: np.ndarray, config: Dict[str, Any]) -> Dict[str, Any]:
    """
    æ·±åº¦ä¼°è®¡ - è¿”å› uint8 å¯è§†åŒ– + float32 åº¦é‡æ·±åº¦(ç±³) + å¤©ç©ºæ©ç 

    è¿”å›:
        dict:
            - depth_map: (H, W) uint8, 0=è¿‘, 255=è¿œ
            - depth_metric: (H, W) float32, å•ä½ç±³ (None if not available)
            - sky_mask: (H, W) bool (None if not available)
    """
    H, W = image.shape[:2]
    depth_backend = str(config.get('depth_backend', 'depth_pro')).lower().strip()

    if depth_backend == 'depth_pro':
        return _depth_estimation_depth_pro(image, config)
    elif depth_backend == 'v3':
        return _depth_estimation_v3(image, config)
    else:
        result = _depth_estimation_v2(image, config)
        # V2 æ²¡æœ‰åº¦é‡æ·±åº¦å’Œå¤©ç©ºæ©ç 
        if isinstance(result, np.ndarray):
            return {'depth_map': result, 'depth_metric': None, 'sky_mask': None}
        return result


def _depth_estimation_depth_pro(image: np.ndarray, config: Dict[str, Any]) -> np.ndarray:
    """
    æ·±åº¦ä¼°è®¡ - ä½¿ç”¨ Apple Depth Pro
    è¾¹ç¼˜é”åˆ©ï¼Œåº¦é‡æ·±åº¦ç²¾ç¡®ï¼Œé€‚åˆè¡—æ™¯
    """
    H, W = image.shape[:2]

    try:
        profile = bool(config.get('profile', False))
        t0 = time.perf_counter()

        depth_model, transform = get_depth_model_depth_pro(config)
        if depth_model is None:
            print(f"  âš ï¸  è­¦å‘Š: Depth Pro æœªå°±ç»ªï¼Œå›é€€åˆ° Depth Anything V3")
            return _depth_estimation_v3(image, config)

        if profile:
            print(f"  â±ï¸  Depth Pro model ready: {time.perf_counter() - t0:.3f}s")

        # OpenCV(BGR)->RGB, ç„¶åè½¬PIL
        rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)
        pil_img = Image.fromarray(rgb)

        # Depth Pro æ¨ç†
        t1 = time.perf_counter()
        device = _get_torch_device(config)
        with _GPU_INFERENCE_LOCK:
            with torch.inference_mode():
                # é¢„å¤„ç†å¹¶ç§»åˆ° GPU
                image_tensor = transform(pil_img)
                if device is not None and device.type == 'cuda':
                    image_tensor = image_tensor.to(device)

                # æ¨ç† - Depth Pro ä¼šè‡ªåŠ¨ä¼°è®¡ç„¦è·
                prediction = depth_model.infer(image_tensor, f_px=None)
                pred = prediction["depth"]  # åº¦é‡æ·±åº¦ (ç±³)
                pred = pred.squeeze().cpu().numpy()

        if profile:
            print(f"  â±ï¸  Depth Pro inference: {time.perf_counter() - t1:.3f}s")

        # å°†é¢„æµ‹ç»“æœ resize å›åŸå›¾å¤§å°
        t2 = time.perf_counter()
        if pred.shape != (H, W):
            pred_resized = cv2.resize(pred, (W, H), interpolation=cv2.INTER_CUBIC)
        else:
            pred_resized = pred

        # Depth Pro è¾“å‡ºåº¦é‡æ·±åº¦: é«˜å€¼=è¿œæ™¯(å¤©ç©º), ä½å€¼=è¿‘æ™¯(åœ°é¢)
        # ä¿ç•™åŸå§‹ç±³æ•°ç”¨äº FMB
        depth_metric = pred_resized.astype(np.float32)

        # å¯è§†åŒ–ç”¨ uint8: 0=è¿‘æ™¯, 255=è¿œæ™¯
        depth_map = _normalize_depth_to_uint8(pred_resized, invert=bool(config.get('depth_invert_depth_pro', False)))

        if profile:
            print(f"  â±ï¸  Depth Pro postprocess: {time.perf_counter() - t2:.3f}s")
            print(f"  ğŸ“ Depth range: {float(depth_metric.min()):.1f}m - {float(depth_metric.max()):.1f}m")

    except Exception as e:
        print(f"  âŒ Depth Pro å‡ºé”™: {e}")
        import traceback
        traceback.print_exc()
        print(f"  âš ï¸  å›é€€åˆ° Depth Anything V3...")
        return _depth_estimation_v3(image, config)

    return {
        'depth_map': depth_map,
        'depth_metric': depth_metric,
        'sky_mask': None,  # Depth Pro æ²¡æœ‰å†…ç½®å¤©ç©ºæ£€æµ‹
    }


def _detect_da3_model_type(config: Dict[str, Any]) -> str:
    """æ£€æµ‹DA3æ¨¡å‹ç±»å‹: 'nested', 'metric', 'mono'"""
    model_id = str(config.get('depth_model_id_v3', '')).upper()
    if 'NESTED' in model_id:
        return 'nested'
    elif 'METRIC' in model_id:
        return 'metric'
    else:
        return 'mono'


def _depth_estimation_v3(image: np.ndarray, config: Dict[str, Any]):
    """
    æ·±åº¦ä¼°è®¡ - ä½¿ç”¨ Depth Anything V3
    æ”¯æŒä¸‰ç§æ¨¡å‹:
      - DA3NESTED: çœŸå®åº¦é‡æ·±åº¦(ç±³) + å†…ç½®å¤©ç©ºæ£€æµ‹ (éœ€16GB+ VRAM)
      - DA3METRIC: è§„èŒƒåŒ–æ·±åº¦ â†’ é€šè¿‡ç„¦è·è½¬æ¢ä¸ºç±³æ•° (æ¨è, 8GB VRAM)
      - DA3MONO: ç›¸å¯¹æ·±åº¦ (æ— ç±³æ•°è¾“å‡º)

    è¿”å›:
        dict: depth_map (uint8), depth_metric (float32 ç±³ or None), sky_mask (bool or None)
    """
    H, W = image.shape[:2]
    model_type = _detect_da3_model_type(config)

    try:
        profile = bool(config.get('profile', False))
        t0 = time.perf_counter()

        depth_model = get_depth_model_v3(config)
        if depth_model is None:
            print(f"  âš ï¸  è­¦å‘Š: Depth Anything V3 æœªå°±ç»ªï¼Œå›é€€åˆ° V2")
            v2_result = _depth_estimation_v2(image, config)
            if isinstance(v2_result, np.ndarray):
                return {'depth_map': v2_result, 'depth_metric': None, 'sky_mask': None}
            return v2_result

        if profile:
            print(f"  â±ï¸  Depth V3 model ready: {time.perf_counter() - t0:.3f}s")

        # å®‰è£… sky mask hook (ä»… DA3Nested æ¨¡å‹æ”¯æŒ)
        if model_type == 'nested':
            _install_sky_hook(depth_model)

        # OpenCV(BGR)->RGB, ç„¶åè½¬PIL
        rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)
        pil_img = Image.fromarray(rgb)

        # æ¨ç†å‚æ•°
        process_res = int(config.get('depth_process_res', 672))

        t1 = time.perf_counter()
        with _GPU_INFERENCE_LOCK:
            with torch.inference_mode():
                prediction = depth_model.inference(
                    [pil_img],
                    export_dir=None,
                    export_format='mini_npz',
                    process_res=process_res,
                    process_res_method='upper_bound_resize',
                )

                # prediction.depth: [N, H, W] float32
                pred = prediction.depth[0]
                pred = pred.cpu().numpy() if hasattr(pred, 'cpu') else np.array(pred)

        if profile:
            print(f"  â±ï¸  Depth V3 inference ({model_type}): {time.perf_counter() - t1:.3f}s")

        # ä»¥ä¸‹å…¨éƒ¨CPUæ“ä½œï¼Œä¸æŒGPUé”ï¼Œå…¶ä»–çº¿ç¨‹å¯ç«‹å³å¼€å§‹GPUæ¨ç†
        t2 = time.perf_counter()
        if pred.shape != (H, W):
            pred_resized = cv2.resize(pred, (W, H), interpolation=cv2.INTER_CUBIC)
        else:
            pred_resized = pred

        # æ ¹æ®æ¨¡å‹ç±»å‹å¤„ç†æ·±åº¦å€¼
        depth_metric = None
        sky_mask = None

        if model_type == 'nested':
            # DA3NESTED: è¾“å‡ºå·²ç»æ˜¯ç±³æ•°
            depth_metric = pred_resized.astype(np.float32)

            # æå–å¤©ç©ºæ©ç  (ç”± hook æ•è·)
            sky_mask_small = _get_sky_mask(depth_model)
            if sky_mask_small is not None:
                sky_mask = cv2.resize(
                    sky_mask_small.astype(np.uint8), (W, H),
                    interpolation=cv2.INTER_NEAREST,
                ).astype(bool)
                sky_pct = sky_mask.sum() / sky_mask.size * 100
                depth_metric[sky_mask] = np.inf
                if profile:
                    print(f"  ğŸŒ¤ï¸  Sky mask (nested): {sky_pct:.1f}% pixels")

        elif model_type == 'metric':
            # DA3METRIC: è¾“å‡ºcanonical depth at focal=300
            # è½¬æ¢: meters = canonical * (actual_focal / 300.0)
            focal_length = float(config.get('depth_focal_length', 300))
            scale = focal_length / 300.0
            depth_metric = (pred_resized * scale).astype(np.float32)
            if profile:
                print(f"  ğŸ“ Focal conversion: canonical * {scale:.3f} (focal={focal_length})")
            # sky_mask ç”± stage2_ai_inference ä» semantic_map æ¨å¯¼

        else:
            # DA3MONO: ç›¸å¯¹æ·±åº¦ï¼Œæ— ç±³æ•°
            depth_metric = None
            if profile:
                print(f"  â„¹ï¸  DA3MONO: relative depth only, no metric output")

        # V2-Style è§†å·®å½’ä¸€åŒ– (å¤©ç©ºè‡ªåŠ¨=255, å¯¹æ¯”åº¦å¢å¼º)
        depth_map = _normalize_depth_v2style(
            pred_resized, sky_mask=sky_mask,
            contrast_boost=float(config.get('depth_contrast_boost', 2.0)),
        )

        if profile:
            if depth_metric is not None:
                non_sky = depth_metric[np.isfinite(depth_metric)]
                if len(non_sky) > 0:
                    print(f"  ğŸ“ Depth range: {float(non_sky.min()):.1f}m - {float(non_sky.max()):.1f}m")
            print(f"  â±ï¸  Depth V3 postprocess: {time.perf_counter() - t2:.3f}s")

    except Exception as e:
        print(f"  âŒ Depth Anything V3 å‡ºé”™: {e}")
        import traceback
        traceback.print_exc()
        print(f"  âš ï¸  å›é€€åˆ° Depth Anything V2...")
        v2_result = _depth_estimation_v2(image, config)
        if isinstance(v2_result, np.ndarray):
            return {'depth_map': v2_result, 'depth_metric': None, 'sky_mask': None}
        return v2_result

    return {
        'depth_map': depth_map,
        'depth_metric': depth_metric,
        'sky_mask': sky_mask,
    }


def _refine_sky_gaps(
    depth_metric: np.ndarray,
    sky_mask: np.ndarray,
    depth_percentile: float = 85,
    dilate_kernel_size: int = 21,
    dilate_iterations: int = 2,
    max_rounds: int = 5,
    image_bgr: np.ndarray | None = None,
    semantic_map: np.ndarray | None = None,
) -> tuple:
    """ä¿®è¡¥å¤©ç©ºç¼éš™: æ ‘ç¼/å»ºç­‘é—´éš™ä¸­çš„å¤©ç©ºåƒç´  (è¿­ä»£æ‰©å±• + é¢œè‰²/è¯­ä¹‰å®ˆå«)ã€‚

    ä¿ç•™åŸæœ‰çš„æ¿€è¿›è†¨èƒ€ç­–ç•¥ï¼ˆ21px/5è½®ï¼‰ä»¥æ·±å…¥æ ‘ç¼ï¼Œ
    ä½†å¢åŠ é¢œè‰²å’Œè¯­ä¹‰å®ˆå«ï¼Œé˜²æ­¢æŠŠç»¿è‰²æ ‘å† ã€æš—è‰²ç‰©ä½“ã€å»ºç­‘ç­‰è¯¯æ ‡ä¸ºå¤©ç©ºã€‚

    å®ˆå«è§„åˆ™:
      - æ’é™¤ç»¿è‰²åƒç´  (H=30-80, S>=40): æ ‘å† /æ¤è¢«
      - æ’é™¤æš—è‰²åƒç´  (V<80): é˜´å½±/æ·±è‰²ç‰©ä½“
      - æ’é™¤ç¡¬æ€§è¯­ä¹‰ç±»: building, road, car ç­‰æ°¸è¿œä¸åº”æ˜¯å¤©ç©ºçš„ç±»åˆ«

    å‚æ•°:
        depth_metric: (H, W) float32, å¤©ç©º=inf
        sky_mask: (H, W) bool, å·²çŸ¥å¤©ç©º
        depth_percentile: æ·±åº¦é˜ˆå€¼ç™¾åˆ†ä½ (é»˜è®¤ 90)
        dilate_kernel_size: è†¨èƒ€æ ¸å¤§å° (é»˜è®¤ 21px)
        dilate_iterations: è†¨èƒ€è¿­ä»£æ¬¡æ•° (é»˜è®¤ 2)
        max_rounds: æœ€å¤§è¿­ä»£è½®æ•° (é»˜è®¤ 5)
        image_bgr: (H, W, 3) uint8 BGRåŸå›¾, ç”¨äºé¢œè‰²å®ˆå«
        semantic_map: (H, W) uint8 ADE20K class ids, ç”¨äºè¯­ä¹‰å®ˆå«

    è¿”å›:
        (refined_sky_mask, total_gap_pixel_count)
    """
    H, W = depth_metric.shape[:2]
    finite_non_sky = depth_metric[np.isfinite(depth_metric) & ~sky_mask]
    if len(finite_non_sky) < 100:
        return sky_mask, 0

    depth_thresh = float(np.percentile(finite_non_sky, depth_percentile))

    # ---- Precompute HSV channels (reused by guard + color detection) ----
    h = s = v = None
    if image_bgr is not None and image_bgr.ndim == 3:
        hsv = cv2.cvtColor(image_bgr, cv2.COLOR_BGR2HSV)
        h, s, v = hsv[:, :, 0], hsv[:, :, 1], hsv[:, :, 2]

    # ---- Build guard mask: pixels that should NEVER become sky ----
    guard = np.zeros((H, W), dtype=bool)

    # Color guard: reject green vegetation and dark pixels
    if h is not None:
        green_veg = (h >= 30) & (h <= 80) & (s >= 40)
        dark = (v < 80)
        guard |= green_veg | dark

    # Semantic guard: hard classes that are never sky
    if semantic_map is not None:
        _NEVER_SKY_IDS = {
            0, 1, 3, 5, 6, 9, 11, 12, 13, 20,  # wall, building, floor, ceiling, road, grass, sidewalk, person, earth, car
            33, 53, 72, 80, 83, 90, 116, 127,    # fence, stairs, signboard, truck, bus, van, pole, bicycle
        }
        for cls_id in _NEVER_SKY_IDS:
            guard |= (semantic_map == cls_id)

    # ---- Pass 1: Color+depth direct detection (no dilation needed) ----
    # Catches isolated sky gaps surrounded by green canopy that dilation
    # can never reach through. Uses stricter depth threshold (p95).
    current_sky = sky_mask.copy()
    total_added = 0

    if h is not None:
        p95 = float(np.percentile(finite_non_sky, 95))

        # Sky-like colors: blue, cyan, gray/overcast, bright white
        blue_sky = (h >= 90) & (h <= 135) & (s >= 20) & (s <= 200) & (v >= 100)
        cyan_sky = (h >= 80) & (h < 90) & (s >= 20) & (s <= 150) & (v >= 140)
        gray_sky = (s <= 25) & (v >= 150) & (v <= 240)
        bright_white = (s <= 30) & (v >= 200)
        sky_color = blue_sky | cyan_sky | gray_sky | bright_white

        # Upper 60% of image only
        upper_mask = np.zeros((H, W), dtype=bool)
        upper_mask[:int(H * 0.6), :] = True

        color_sky = (
            sky_color
            & upper_mask
            & (depth_metric > p95)
            & ~current_sky
            & ~guard
        )
        color_added = int(color_sky.sum())
        if color_added > 0:
            current_sky |= color_sky
            total_added += color_added

    # ---- Pass 2: Iterative dilation (aggressive reach into gaps) ----
    kernel = cv2.getStructuringElement(
        cv2.MORPH_ELLIPSE, (dilate_kernel_size, dilate_kernel_size))

    for _round in range(max_rounds):
        sky_nearby = cv2.dilate(
            current_sky.astype(np.uint8) * 255, kernel,
            iterations=dilate_iterations,
        ) > 0

        gap_pixels = (
            (depth_metric > depth_thresh)
            & sky_nearby
            & ~current_sky
            & ~guard
        )
        added = int(gap_pixels.sum())
        if added == 0:
            break

        current_sky = current_sky | gap_pixels
        total_added += added

    return current_sky, total_added


def _install_sky_hook(depth_model):
    """
    ç»™ DA3Nested æ¨¡å‹å®‰è£… hookï¼Œæ•è· sky maskã€‚
    DA3Nested._handle_sky_regions å†…éƒ¨è®¡ç®—äº†å¤©ç©ºæ©ç ä½†æ²¡æœ‰æš´éœ²ï¼Œ
    æˆ‘ä»¬é€šè¿‡ monkey-patch æŠŠå®ƒå­˜ä¸‹æ¥ã€‚
    """
    inner_model = getattr(depth_model, 'model', None)
    if inner_model is None:
        return

    # åªå¤„ç† DA3Nested ç±»å‹
    cls_name = type(inner_model).__name__
    if 'Nested' not in cls_name:
        return

    # å¦‚æœå·²ç» patch è¿‡å°±è·³è¿‡
    if getattr(inner_model, '_sky_hook_installed', False):
        return

    original_handle_sky = inner_model._handle_sky_regions

    def _patched_handle_sky(output, metric_output, sky_depth_def=200.0):
        from depth_anything_3.utils.alignment import compute_sky_mask
        non_sky_mask = compute_sky_mask(metric_output.sky, threshold=0.3)
        # å­˜å‚¨å¤©ç©ºæ©ç : True = å¤©ç©º
        sky_mask = ~non_sky_mask  # (B, S, H, W) bool tensor
        inner_model._captured_sky_mask = sky_mask.squeeze().cpu().numpy()
        return original_handle_sky(output, metric_output, sky_depth_def)

    inner_model._handle_sky_regions = _patched_handle_sky
    inner_model._sky_hook_installed = True
    inner_model._captured_sky_mask = None


def _get_sky_mask(depth_model) -> np.ndarray | None:
    """ä» hook ä¸­è·å–å¤©ç©ºæ©ç """
    inner_model = getattr(depth_model, 'model', None)
    if inner_model is None:
        return None
    mask = getattr(inner_model, '_captured_sky_mask', None)
    return mask


def _depth_estimation_v2(image: np.ndarray, config: Dict[str, Any]) -> np.ndarray:
    """
    æ·±åº¦ä¼°è®¡ - ä½¿ç”¨ Depth Anything V2 (åŸæœ‰å®ç°)
    """
    H, W = image.shape[:2]
    
    try:
        profile = bool(config.get('profile', False))
        t0 = time.perf_counter()

        depth_model, depth_processor = get_depth_model(config)
        if depth_model is None or depth_processor is None:
            print(f"  âš ï¸  è­¦å‘Š: Depth Anything V2 æœªå°±ç»ªï¼Œä½¿ç”¨å ä½æ·±åº¦å›¾")
            return _generate_placeholder_depth_map(H, W)

        if profile:
            print(f"  â±ï¸  Depth model ready: {time.perf_counter() - t0:.3f}s")

        device = _get_torch_device(config)
        use_fp16 = bool(config.get('depth_use_fp16', True))

        # OpenCV(BGR)->RGB
        rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)

        # æ¨ç†
        t1 = time.perf_counter()
        with _GPU_INFERENCE_LOCK:
            with torch.inference_mode():
                inputs = depth_processor(images=rgb, return_tensors='pt')
                inputs = {k: v.to(device) for k, v in inputs.items()}
                if use_fp16 and device.type == 'cuda':
                    inputs = {k: v.half() if torch.is_floating_point(v) else v for k, v in inputs.items()}

                outputs = depth_model(**inputs)

                # transformers çš„æ·±åº¦æ¨¡å‹ä¸€èˆ¬æœ‰ predicted_depth
                if hasattr(outputs, 'predicted_depth'):
                    pred = outputs.predicted_depth
                elif isinstance(outputs, (tuple, list)) and len(outputs) > 0:
                    pred = outputs[0]
                else:
                    pred = getattr(outputs, 'logits', None)

                if pred is None:
                    raise RuntimeError("Depth Anything V2 è¾“å‡ºä¸åŒ…å« predicted_depth/logits")

                # pred: (B, H', W')
                pred = pred.squeeze(0)
                pred = pred.detach().float().cpu().numpy()

        if profile:
            print(f"  â±ï¸  Depth inference: {time.perf_counter() - t1:.3f}s")

        # å°†é¢„æµ‹ç»“æœ resize å›åŸå›¾å¤§å°
        t2 = time.perf_counter()
        pred_resized = cv2.resize(pred, (W, H), interpolation=cv2.INTER_CUBIC)

        depth_map = _normalize_depth_to_uint8(pred_resized, invert=bool(config.get('depth_invert', True)))

        if profile:
            print(f"  â±ï¸  Depth postprocess (resize+norm): {time.perf_counter() - t2:.3f}s")
        
    except ImportError as e:
        print(f"  âŒ é”™è¯¯: ç¼ºå°‘å¿…è¦çš„åº“ï¼Œæ— æ³•æ‰§è¡Œæ·±åº¦ä¼°è®¡")
        print(f"  è¯·å®‰è£…: pip install transformers accelerate")
        print(f"  é”™è¯¯è¯¦æƒ…: {e}")
        # è¿”å›ç®€å•æ¢¯åº¦å›¾ï¼Œä¸ä¸­æ–­æµç¨‹
        return _generate_placeholder_depth_map(H, W)
    
    except Exception as e:
        print(f"  âŒ æ·±åº¦ä¼°è®¡å‡ºé”™: {e}")
        import traceback
        traceback.print_exc()
        # è¿”å›ç®€å•æ¢¯åº¦å›¾ï¼Œä¸ä¸­æ–­æµç¨‹
        return _generate_placeholder_depth_map(H, W)
    
    return depth_map


def _normalize_depth_to_uint8(depth_raw: np.ndarray, invert: bool = True) -> np.ndarray:
    """æŠŠ float æ·±åº¦/é€†æ·±åº¦å›¾å½’ä¸€åŒ–åˆ° uint8 [0,255]ã€‚

    çº¦å®šè¾“å‡ºï¼š0=è¿‘ï¼Œ255=è¿œã€‚
    Depth Anything V2 çš„è¾“å‡ºé€šå¸¸æ›´åƒ"é€†æ·±åº¦"ï¼ˆè¿‘å¤„æ›´å¤§ï¼‰ï¼Œå› æ­¤é»˜è®¤ invert=Trueã€‚
    """
    depth_min = float(np.min(depth_raw))
    depth_max = float(np.max(depth_raw))
    if not np.isfinite(depth_min) or not np.isfinite(depth_max) or depth_max == depth_min:
        return np.full(depth_raw.shape, 128, dtype=np.uint8)

    norm = (depth_raw - depth_min) / (depth_max - depth_min)
    norm = (norm * 255.0).clip(0, 255).astype(np.uint8)
    if invert:
        norm = (255 - norm).astype(np.uint8)
    return norm


def _normalize_depth_v2style(depth_raw: np.ndarray, sky_mask: np.ndarray | None = None,
                              contrast_boost: float = 2.0) -> np.ndarray:
    """V2-Style è§†å·®å½’ä¸€åŒ– (å‚è€ƒ ComfyUI-DepthAnythingV3)ã€‚

    çº¯numpy/cv2å®ç°ï¼Œé¿å…GPUä¼ è¾“å¼€é”€ã€‚
    å¯¹äº<1Måƒç´ çš„å›¾åƒï¼Œnumpyåœ¨7800X3D V-cacheä¸Šæ¯”GPU+PCIeä¼ è¾“æ›´å¿«ã€‚

    æµç¨‹:
        1. depth â†’ disparity (1/depth)ï¼Œå¤©ç©ºè‡ªç„¶å˜ä¸º~0
        2. ç”¨ sky_mask æ’é™¤å¤©ç©ºåŒºåŸŸï¼Œä»…å¯¹å†…å®¹åŒºåŸŸå½’ä¸€åŒ–
        3. ç™¾åˆ†ä½è£å‰ª (1%-99%) æå‡å¯¹æ¯”åº¦
        4. power transform å¢å¼ºè¿‘æ™¯ç»†èŠ‚
        5. å¤©ç©ºè¾¹ç¼˜æŠ—é”¯é½¿
        6. åè½¬è¾“å‡ºä½¿ 0=è¿‘, 255=è¿œ

    è¿”å›:
        depth_map: (H, W) uint8, 0=è¿‘, 255=è¿œ (å¤©ç©º=255)
    """
    epsilon = 1e-6

    disparity = 1.0 / (depth_raw.astype(np.float64) + epsilon)

    if sky_mask is not None and sky_mask.any():
        content_mask = (~sky_mask).astype(np.float32)
    else:
        content_mask = np.ones_like(disparity, dtype=np.float32)

    content_pixels = disparity[content_mask > 0.5]
    if content_pixels.size > 100:
        p1 = np.percentile(content_pixels, 1)
        p99 = np.percentile(content_pixels, 99)
        if p99 - p1 < epsilon:
            p1 = content_pixels.min()
            p99 = content_pixels.max()
    else:
        p1 = disparity.min()
        p99 = disparity.max()

    disp_norm = (disparity - p1) / (p99 - p1 + epsilon)
    disp_norm = np.clip(disp_norm, 0.0, 1.0)
    disp_contrast = np.power(disp_norm, 1.0 / contrast_boost)

    if sky_mask is not None and sky_mask.any():
        content_smooth = cv2.blur(content_mask, (3, 3))
        disp_contrast = disp_contrast * content_smooth

    result = 1.0 - disp_contrast
    return (result * 255.0).clip(0, 255).astype(np.uint8)


def _generate_placeholder_semantic_map(semantic_map: np.ndarray, classes: List[str], H: int, W: int) -> None:
    """
    ç”Ÿæˆå ä½è¯­ä¹‰å›¾ (ä»…ç”¨äºæµ‹è¯•)
    ç®€å•çš„å‚ç›´åˆ†å—æ¨¡å¼
    """
    num_classes = len(classes)
    if num_classes == 0:
        return
    
    # ç®€å•çš„å‚ç›´åˆ†å—
    block_height = H // num_classes
    for i, class_id in enumerate(range(1, num_classes + 1)):
        y_start = i * block_height
        y_end = (i + 1) * block_height if i < num_classes - 1 else H
        semantic_map[y_start:y_end, :] = class_id


def _generate_placeholder_depth_map(H: int, W: int) -> np.ndarray:
    """
    ç”Ÿæˆå ä½æ·±åº¦å›¾ (ä»…ç”¨äºæµ‹è¯•)
    ç®€å•çš„å‚ç›´æ¢¯åº¦
    """
    depth_map = np.zeros((H, W), dtype=np.uint8)
    for y in range(H):
        depth_map[y, :] = int((y / H) * 255)
    return depth_map


# ============================================
# æ¨¡å‹ç¼“å­˜ç®¡ç† (ç”¨äºä¼˜åŒ–æ€§èƒ½)
# ============================================

_model_cache = {
    'semantic_model': None,
    'semantic_processor': None,
    'semantic_backend': None,
    'semantic_device': None,
    'depth_model': None,
    'depth_processor': None,
    'depth_model_v3': None,  # Depth Anything V3 æ¨¡å‹
    'depth_model_depth_pro': None,  # Apple Depth Pro æ¨¡å‹
    'depth_transform_depth_pro': None,  # Depth Pro é¢„å¤„ç† transform
}


def get_semantic_model(config: Dict[str, Any]):
    """
    è·å–è¯­ä¹‰åˆ†å‰²æ¨¡å‹ (å¸¦ç¼“å­˜, çº¿ç¨‹å®‰å…¨)
    åªåˆå§‹åŒ–ä¸€æ¬¡ï¼Œåç»­å¤ç”¨
    """
    if not TORCH_AVAILABLE:
        return None, None

    backend = str(config.get('semantic_backend', 'oneformer_ade20k')).strip().lower()

    # å¿«é€Ÿè·¯å¾„: å·²ç¼“å­˜ä¸”backendç›¸åŒ
    if (_model_cache['semantic_model'] is not None
            and _model_cache.get('semantic_backend') == backend):
        return _model_cache['semantic_model'], _model_cache['semantic_processor']

    # æ…¢è·¯å¾„: åŠ é”åŠ è½½
    with _MODEL_LOAD_LOCK:
        # Double-check
        if (_model_cache['semantic_model'] is not None
                and _model_cache.get('semantic_backend') == backend):
            return _model_cache['semantic_model'], _model_cache['semantic_processor']

        device = _get_torch_device(config)

        # If backend changes between calls, reset semantic cache.
        if _model_cache.get('semantic_backend') != backend:
            _model_cache['semantic_model'] = None
            _model_cache['semantic_processor'] = None
            _model_cache['semantic_backend'] = backend
            _model_cache['semantic_device'] = None

        if backend.startswith('oneformer'):
            try:
                profile = bool(config.get('profile', False))
                t_import = time.perf_counter()
                print("  åˆå§‹åŒ– OneFormerï¼ˆé¦–æ¬¡å¯¼å…¥/ä¸‹è½½æƒé‡å¯èƒ½è¾ƒæ…¢ï¼‰...")
                from transformers import AutoProcessor, OneFormerForUniversalSegmentation
                if profile:
                    print(f"  â±ï¸  import transformers(oneformer): {time.perf_counter() - t_import:.3f}s")
            except Exception as e:
                print(f"  âŒ æ— æ³•å¯¼å…¥ OneFormer ç›¸å…³ä¾èµ–: {e}")
                return None, None

            model_id = str(config.get('oneformer_model_id', 'shi-labs/oneformer_ade20k_swin_large'))
            use_fp16 = bool(config.get('semantic_use_fp16', True))
            print(f"  åŠ è½½ OneFormer(ADE20K-150): {model_id} (device={device}, fp16={use_fp16})")

            t_load = time.perf_counter()
            processor = AutoProcessor.from_pretrained(model_id)
            model = OneFormerForUniversalSegmentation.from_pretrained(model_id)
            model.eval()

            if device.type == 'cuda':
                model.to(device)
                if use_fp16:
                    model.half()

            if bool(config.get('profile', False)):
                print(f"  â±ï¸  load processor+model: {time.perf_counter() - t_load:.3f}s")

            _model_cache['semantic_model'] = model
            _model_cache['semantic_processor'] = processor
            _model_cache['semantic_backend'] = backend
            _model_cache['semantic_device'] = str(device)
            return model, processor

        # LangSAM backend
        try:
            profile = bool(config.get('profile', False))
            t_import = time.perf_counter()
            print("  åˆå§‹åŒ– LangSAMï¼ˆé¦–æ¬¡å¯¼å…¥/ä¸‹è½½æƒé‡å¯èƒ½è¾ƒæ…¢ï¼‰...")
            from lang_sam import LangSAM
            if profile:
                print(f"  â±ï¸  import lang_sam: {time.perf_counter() - t_import:.3f}s")
        except Exception as e:
            print(f"  âŒ æ— æ³•å¯¼å…¥ lang_sam: {e}")
            return None, None

        sam_type = str(config.get('sam_type', '')).strip()
        if not sam_type:
            encoder = str(config.get('encoder', 'vitb')).lower()
            if encoder in ('vitb', 'vit_b', 'b'):
                sam_type = 'sam2.1_hiera_base_plus'
            elif encoder in ('vits', 'vit_s', 's'):
                sam_type = 'sam2.1_hiera_small'
            else:
                sam_type = 'sam2.1_hiera_small'

        print(f"  åŠ è½½ LangSAM (sam_type={sam_type}, device={device})")
        model = LangSAM(sam_type=sam_type, device=device)

        _model_cache['semantic_model'] = model
        _model_cache['semantic_processor'] = None
        _model_cache['semantic_backend'] = backend
        _model_cache['semantic_device'] = str(device)
        return model, None


def _langsam_predict_masks(model, pil_img: Image.Image, text_prompt: str, config: Dict[str, Any] | None = None) -> Optional[np.ndarray]:
    """è°ƒç”¨ LangSAM å¹¶è¿”å› masks ndarrayã€‚

    å…¼å®¹ä¸åŒç‰ˆæœ¬è¿”å›æ ¼å¼ï¼šå¯èƒ½æ˜¯ (masks, boxes, phrases, logits) æˆ– dictã€‚
    è¿”å›:
      - None: æ²¡æœ‰æ£€æµ‹åˆ°
      - np.ndarray: (N,H,W) æˆ– (H,W)
    """
    cfg = config or {}
    box_threshold = float(cfg.get('box_threshold', 0.3))
    text_threshold = float(cfg.get('text_threshold', 0.25))

    # LangSAM/SAM2 predictor is not thread-safe; serialize predict calls.
    with _GPU_INFERENCE_LOCK:
        # LangSAM.predict expects lists and returns list[dict] (one per image)
        results = model.predict([pil_img], [text_prompt], box_threshold=box_threshold, text_threshold=text_threshold)
    if not results:
        return None
    first = results[0]
    if not isinstance(first, dict):
        return None

    masks = first.get('masks', None)
    if masks is None:
        return None

    masks = np.asarray(masks)
    if masks.size == 0:
        return None
    return masks


def get_depth_model(config: Dict[str, Any]):
    """
    è·å–æ·±åº¦ä¼°è®¡æ¨¡å‹ (å¸¦ç¼“å­˜)
    åªåˆå§‹åŒ–ä¸€æ¬¡ï¼Œåç»­å¤ç”¨
    """
    if _model_cache['depth_model'] is None:
        if not TORCH_AVAILABLE:
            return None, None

        try:
            # è¿™ä¸€æ®µåœ¨é¦–æ¬¡è¿è¡Œæ—¶å¯èƒ½ä¼šâ€œå¡ä½â€ï¼Œé€šå¸¸æ˜¯ transformers/torch çš„å¤§å¯¼å…¥ + CUDA åˆå§‹åŒ–ï¼Œå¹¶é bugã€‚
            profile = bool(config.get('profile', False))
            t_import = time.perf_counter()
            print("  åˆå§‹åŒ– Depth Anything V2ï¼ˆå¯¼å…¥ transformersï¼Œé¦–æ¬¡å¯èƒ½è¾ƒæ…¢ï¼‰...")
            from transformers import AutoImageProcessor, AutoModelForDepthEstimation
            if profile:
                print(f"  â±ï¸  import transformers: {time.perf_counter() - t_import:.3f}s")
        except Exception as e:
            print(f"  âŒ æ— æ³•å¯¼å…¥ transformers æ·±åº¦æ¨¡å‹: {e}")
            return None, None

        model_id = str(config.get('depth_model_id', 'depth-anything/Depth-Anything-V2-Base-hf'))
        device = _get_torch_device(config)
        use_fp16 = bool(config.get('depth_use_fp16', True))

        print(f"  åŠ è½½ Depth Anything V2: {model_id} (device={device}, fp16={use_fp16})")

        t_load = time.perf_counter()
        # transformers ä¼šæç¤º future default use_fast=Trueï¼›è¿™é‡Œå°½é‡æ˜¾å¼æŒ‡å®šã€‚
        try:
            processor = AutoImageProcessor.from_pretrained(model_id, use_fast=True)
        except TypeError:
            processor = AutoImageProcessor.from_pretrained(model_id)
        model = AutoModelForDepthEstimation.from_pretrained(model_id)
        if profile:
            print(f"  â±ï¸  load processor+model: {time.perf_counter() - t_load:.3f}s")
        model.eval()

        if device.type == 'cuda':
            t_to = time.perf_counter()
            model.to(device)
            if use_fp16:
                model.half()
            if profile:
                print(f"  â±ï¸  model.to(device)/half: {time.perf_counter() - t_to:.3f}s")

        _model_cache['depth_model'] = model
        _model_cache['depth_processor'] = processor

    return _model_cache['depth_model'], _model_cache['depth_processor']


def get_depth_model_v3(config: Dict[str, Any]):
    """
    è·å– Depth Anything V3 æ¨¡å‹ (å¸¦ç¼“å­˜, çº¿ç¨‹å®‰å…¨)
    """
    # å¿«é€Ÿè·¯å¾„: å·²ç¼“å­˜åˆ™ç›´æ¥è¿”å›
    if _model_cache['depth_model_v3'] is not None:
        return _model_cache['depth_model_v3']

    # æ…¢è·¯å¾„: åŠ é”åŠ è½½ï¼Œé˜²æ­¢å¤šçº¿ç¨‹åŒæ—¶åŠ è½½å¤§æ¨¡å‹å¯¼è‡´OOM
    with _MODEL_LOAD_LOCK:
        # Double-check: å¦ä¸€ä¸ªçº¿ç¨‹å¯èƒ½å·²ç»åŠ è½½å®Œæˆ
        if _model_cache['depth_model_v3'] is not None:
            return _model_cache['depth_model_v3']

        if not TORCH_AVAILABLE:
            return None

        try:
            profile = bool(config.get('profile', False))
            t_import = time.perf_counter()
            print("  åˆå§‹åŒ– Depth Anything V3...")
            from depth_anything_3.api import DepthAnything3
            if profile:
                print(f"  â±ï¸  import depth_anything_3: {time.perf_counter() - t_import:.3f}s")
        except ImportError as e:
            print(f"  âŒ æ— æ³•å¯¼å…¥ depth_anything_3: {e}")
            print(f"  è¯·å®‰è£…: pip install git+https://github.com/ByteDance-Seed/depth-anything-3.git")
            return None

        # æ”¯æŒçš„æ¨¡å‹:
        #   - depth-anything/DA3NESTED-GIANT-LARGE-1.1 : åº¦é‡æ·±åº¦(ç±³) + å¤©ç©ºæ£€æµ‹ (æ¨è)
        #   - depth-anything/DA3NESTED-GIANT-LARGE     : åº¦é‡æ·±åº¦(ç±³) + å¤©ç©ºæ£€æµ‹
        #   - depth-anything/DA3METRIC-LARGE           : è§„èŒƒåŒ–æ·±åº¦ (éœ€è¦ç„¦è·è½¬æ¢)
        #   - depth-anything/DA3MONO-LARGE             : ç›¸å¯¹æ·±åº¦ (éåº¦é‡)
        model_id = str(config.get('depth_model_id_v3', 'depth-anything/DA3NESTED-GIANT-LARGE-1.1'))
        device = _get_torch_device(config)

        print(f"  åŠ è½½ Depth Anything V3: {model_id} (device={device})")

        t_load = time.perf_counter()
        try:
            model = DepthAnything3.from_pretrained(model_id)
            model = model.to(device=device)
            model.eval()
        except Exception as e:
            print(f"  âŒ åŠ è½½ Depth Anything V3 å¤±è´¥: {e}")
            import traceback
            traceback.print_exc()
            return None

        if profile:
            print(f"  â±ï¸  load DA3 model: {time.perf_counter() - t_load:.3f}s")

        _model_cache['depth_model_v3'] = model

    return _model_cache['depth_model_v3']


def get_depth_model_depth_pro(config: Dict[str, Any]):
    """
    è·å– Apple Depth Pro æ¨¡å‹ (å¸¦ç¼“å­˜)
    è¾¹ç¼˜é”åˆ©ï¼Œåº¦é‡æ·±åº¦ç²¾ç¡®ï¼Œé€‚åˆè¡—æ™¯
    """
    if _model_cache['depth_model_depth_pro'] is None:
        if not TORCH_AVAILABLE:
            return None, None

        try:
            profile = bool(config.get('profile', False))
            t_import = time.perf_counter()
            print("  åˆå§‹åŒ– Depth Pro (Apple)...")
            import depth_pro
            if profile:
                print(f"  â±ï¸  import depth_pro: {time.perf_counter() - t_import:.3f}s")
        except ImportError as e:
            print(f"  âŒ æ— æ³•å¯¼å…¥ depth_pro: {e}")
            print(f"  è¯·å®‰è£…: pip install git+https://github.com/apple/ml-depth-pro.git")
            print(f"  ç„¶åè¿è¡Œ: source get_pretrained_models.sh ä¸‹è½½æƒé‡")
            return None, None

        device = _get_torch_device(config)

        print(f"  åŠ è½½ Depth Pro (device={device})")

        t_load = time.perf_counter()
        try:
            model, transform = depth_pro.create_model_and_transforms()
            model = model.to(device)
            model.eval()
        except Exception as e:
            print(f"  âŒ åŠ è½½ Depth Pro å¤±è´¥: {e}")
            import traceback
            traceback.print_exc()
            return None, None

        if profile:
            print(f"  â±ï¸  load Depth Pro model: {time.perf_counter() - t_load:.3f}s")

        _model_cache['depth_model_depth_pro'] = model
        _model_cache['depth_transform_depth_pro'] = transform

    return _model_cache['depth_model_depth_pro'], _model_cache['depth_transform_depth_pro']


def _get_torch_device(config: Dict[str, Any]):
    if not TORCH_AVAILABLE:
        return None
    device_str = str(config.get('device', 'cuda' if torch.cuda.is_available() else 'cpu'))
    return torch.device(device_str)


def clear_model_cache():
    """
    æ¸…é™¤æ¨¡å‹ç¼“å­˜ (é‡Šæ”¾GPUå†…å­˜)
    """
    global _model_cache
    _model_cache['semantic_model'] = None
    _model_cache['semantic_processor'] = None
    _model_cache['semantic_backend'] = None
    _model_cache['semantic_device'] = None
    _model_cache['depth_model'] = None
    _model_cache['depth_processor'] = None
    _model_cache['depth_model_v3'] = None
    _model_cache['depth_model_depth_pro'] = None
    _model_cache['depth_transform_depth_pro'] = None
    if TORCH_AVAILABLE and torch.cuda.is_available():
        torch.cuda.empty_cache()
